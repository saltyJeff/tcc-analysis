{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca2c0600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.12\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.0\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.11.12, OpenJDK 64-Bit Server VM, 1.8.0_312\n",
      "Branch \n",
      "Compiled by user  on 2018-10-29T06:22:05Z\n",
      "Revision \n",
      "Url \n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bfef80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set settings here\n",
    "BUCKET = \"tcceval-data\"\n",
    "DATABASE_CACHE_FOLDER = \"_db_cache\"\n",
    "DUMP_PARQUET_FOLDER = \"craigslist_parquet\"\n",
    "DUMP_EXCEL_FOLDER = \"craigslist_excel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6833dc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/aws-java-sdk-core-1.11.835.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/aws-java-sdk-kms-1.11.835.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/aws-java-sdk-s3-1.11.835.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/aws-java-sdk-sagemaker-1.11.835.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/aws-java-sdk-sagemakerruntime-1.11.835.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/aws-java-sdk-sts-1.11.835.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/hadoop-annotations-2.8.1.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/hadoop-auth-2.8.1.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/hadoop-aws-2.8.1.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/hadoop-common-2.8.1.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/htrace-core4-4.0.1-incubating.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/sagemaker-spark_2.11-spark_2.4.0-1.4.2.dev0.jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.sedona#sedona-python-adapter-2.4_2.11 added as a dependency\n",
      "org.apache.sedona#sedona-viz-2.4_2.11 added as a dependency\n",
      "org.datasyslab#geotools-wrapper added as a dependency\n",
      "mysql#mysql-connector-java added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c6d7a0c3-4411-48dd-8f1b-b68c9d0b1206;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.sedona#sedona-python-adapter-2.4_2.11;1.2.0-incubating in central\n",
      "\tfound org.locationtech.jts#jts-core;1.18.0 in central\n",
      "\tfound org.wololo#jts2geojson;0.16.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.12.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.12.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.12.2 in central\n",
      "\tfound org.apache.sedona#sedona-core-2.4_2.11;1.2.0-incubating in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.5.0 in central\n",
      "\tfound org.apache.sedona#sedona-sql-2.4_2.11;1.2.0-incubating in central\n",
      "\tfound org.apache.sedona#sedona-viz-2.4_2.11;1.2.0-incubating in central\n",
      "\tfound org.beryx#awt-color-factory;1.0.0 in central\n",
      "\tfound org.datasyslab#geotools-wrapper;1.1.0-25.2 in central\n",
      "\tfound mysql#mysql-connector-java;8.0.30 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.19.4 in central\n",
      ":: resolution report :: resolve 554ms :: artifacts dl 13ms\n",
      "\t:: modules in use:\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.12.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.12.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.12.2 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.19.4 from central in [default]\n",
      "\tmysql#mysql-connector-java;8.0.30 from central in [default]\n",
      "\torg.apache.sedona#sedona-core-2.4_2.11;1.2.0-incubating from central in [default]\n",
      "\torg.apache.sedona#sedona-python-adapter-2.4_2.11;1.2.0-incubating from central in [default]\n",
      "\torg.apache.sedona#sedona-sql-2.4_2.11;1.2.0-incubating from central in [default]\n",
      "\torg.apache.sedona#sedona-viz-2.4_2.11;1.2.0-incubating from central in [default]\n",
      "\torg.beryx#awt-color-factory;1.0.0 from central in [default]\n",
      "\torg.datasyslab#geotools-wrapper;1.1.0-25.2 from central in [default]\n",
      "\torg.locationtech.jts#jts-core;1.18.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.5.0 from central in [default]\n",
      "\torg.wololo#jts2geojson;0.16.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.locationtech.jts#jts-core;1.18.1 by [org.locationtech.jts#jts-core;1.18.0] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   15  |   0   |   0   |   1   ||   14  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c6d7a0c3-4411-48dd-8f1b-b68c9d0b1206\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 14 already retrieved (0kB/13ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-05 18:11:11 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-05 18:11:12 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2022-08-05 18:11:12 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Establishes Spark session\n",
    "ADDITIONAL_ARTIFACTS = [\n",
    "    # Apache Sedona handles spatial queries\n",
    "    'org.apache.sedona:sedona-python-adapter-2.4_2.11:1.2.0-incubating',\n",
    "    'org.apache.sedona:sedona-viz-2.4_2.11:1.2.0-incubating',\n",
    "    'org.datasyslab:geotools-wrapper:1.1.0-25.2',\n",
    "    # JDBC connector for MySQL\n",
    "    'mysql:mysql-connector-java:8.0.30'\n",
    "]\n",
    "\n",
    "import sagemaker_pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "print(classpath)\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.extraClassPath\", classpath) \\\n",
    "    .config(\"spark.executor.extraClassPath\", classpath) \\\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName) \\\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) \\\n",
    "    .config('spark.jars.packages',\n",
    "           ','.join(ADDITIONAL_ARTIFACTS)) \\\n",
    "    .getOrCreate()\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "# suppresses a bunch of AbortableS3Stream warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c764a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pid: long (nullable = true)\n",
      " |-- repostid: string (nullable = true)\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- neighb: string (nullable = true)\n",
      " |-- beds: long (nullable = true)\n",
      " |-- sqft: long (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lng: double (nullable = true)\n",
      " |-- accuracy: double (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- posttext: string (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- group: long (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- geoid: string (nullable = true)\n",
      " |-- url_type: string (nullable = true)\n",
      " |-- is_adu: boolean (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- pct_price: double (nullable = true)\n",
      " |-- pct_sqft: double (nullable = true)\n",
      "\n",
      "+----------+----------+-------------------+--------------------+--------------------+------+--------------------+----+----+---------+-----------+--------+--------------------+--------------------+--------------------+-----+-------+-----------+--------+------+----+-------------------+-------------------+\n",
      "|       pid|  repostid|                 dt|                 url|               title| price|              neighb|beds|sqft|      lat|        lng|accuracy|             address|            posttext|              domain|group| region|      geoid|url_type|is_adu|type|          pct_price|           pct_sqft|\n",
      "+----------+----------+-------------------+--------------------+--------------------+------+--------------------+----+----+---------+-----------+--------+--------------------+--------------------+--------------------+-----+-------+-----------+--------+------+----+-------------------+-------------------+\n",
      "|7226987246|          |2020-11-16 21:29:00|https://inlandemp...|Una casita (backh...|1500.0|          Ontario CA|   1| 750|34.059511|-117.670188|    20.0|Mountain Ave near...|Una casita (backh...|https://inlandemp...|    1|ontario|06071001600|     apa|  true| adu| 0.3076923076923077|0.23076923076923078|\n",
      "|7466590703|6852445542|2022-04-03 14:36:00|https://inlandemp...|Studio de renta/k...| 965.0|G Mountain/Holt/1...|   0| 300|  34.0584|  -117.6665|    22.0|                    |Disponible para u...|https://inlandemp...|    1|ontario|06071001600|     apa|  true| adu|0.19230769230769232|0.07692307692307693|\n",
      "|7434963978|          |2022-01-22 09:07:00|https://inlandemp...|Una casita (backh...|1800.0|          Ontario CA|   1| 750|34.059511|-117.670188|    20.0|Mountain Ave near...|Una casita (backh...|https://inlandemp...|    1|ontario|06071001600|     apa|  true| adu| 0.4230769230769231|0.23076923076923078|\n",
      "|7165113645|          |2020-07-24 09:21:00|https://inlandemp...|Master bedroom wi...| 700.0|             Fontana|   0| 550|   34.086|   -117.462|    99.0|                    |Master bedroom fo...|https://inlandemp...|    0|ontario|06071002402|     roo| false| roo| 0.4712041884816754| 0.6649214659685864|\n",
      "|7236145481|6951273582|2020-11-23 21:17:00|https://inlandemp...|Room for rent plu...| 650.0|              Rialto|   0| 240|34.108209|-117.361678|    10.0|515 N Acacia Ave ...|Hi Boys...my part...|https://inlandemp...|    0|ontario|06071003803|     roo| false| roo| 0.3193717277486911| 0.4816753926701571|\n",
      "+----------+----------+-------------------+--------------------+--------------------+------+--------------------+----+----+---------+-----------+--------+--------------------+--------------------+--------------------+-----+-------+-----------+--------+------+----+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 5.2 ms, sys: 391 µs, total: 5.59 ms\n",
      "Wall time: 3.29 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Est. 50 sec\n",
    "# load in the craigslist data\n",
    "craigslist = spark.read.parquet(f\"s3a://{BUCKET}/{DUMP_PARQUET_FOLDER}/\")\n",
    "craigslist.printSchema()\n",
    "craigslist.createOrReplaceTempView(\"craigslist\")\n",
    "craigslist.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc0389c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- med_price: double (nullable = true)\n",
      " |-- med_sqft: double (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- geoid: string (nullable = true)\n",
      " |-- time: integer (nullable = false)\n",
      " |-- group: long (nullable = true)\n",
      " |-- post_treatment: long (nullable = true)\n",
      " |-- med_price_per_sqft: double (nullable = true)\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) Project [med_price#1363, med_sqft#1364, region#1366, geoid#28, time#1312, group#26L, post_treatment#1337L, (med_price#1363 / med_sqft#1364) AS med_price_per_sqft#1376]\n",
      "+- ObjectHashAggregate(keys=[geoid#28, time#1312, group#26L, post_treatment#1337L], functions=[percentile(price#16, 0.5, 1, 0, 0), percentile(sqft#19L, 0.5, 1, 0, 0), first(region#27, false)])\n",
      "   +- Exchange hashpartitioning(geoid#28, time#1312, group#26L, post_treatment#1337L, 200)\n",
      "      +- ObjectHashAggregate(keys=[geoid#28, time#1312, group#26L, post_treatment#1337L], functions=[partial_percentile(price#16, 0.5, 1, 0, 0), partial_percentile(sqft#19L, 0.5, 1, 0, 0), partial_first(region#27, false)])\n",
      "         +- *(1) Project [price#16, sqft#19L, group#26L, region#27, geoid#28, CASE WHEN (dt#13 < 1583020800000000) THEN 0 WHEN (dt#13 > 1630454400000000) THEN 1 ELSE -1 END AS time#1312, (cast(CASE WHEN (dt#13 < 1583020800000000) THEN 0 WHEN (dt#13 > 1630454400000000) THEN 1 ELSE -1 END as bigint) * group#26L) AS post_treatment#1337L]\n",
      "            +- *(1) Filter ((((((isnotnull(pct_price#32) && isnotnull(pct_sqft#33)) && (pct_price#32 > 0.02)) && (pct_price#32 < 0.98)) && (pct_sqft#33 > 0.02)) && (pct_sqft#33 < 0.98)) && NOT (CASE WHEN (dt#13 < 1583020800000000) THEN 0 WHEN (dt#13 > 1630454400000000) THEN 1 ELSE -1 END = -1))\n",
      "               +- *(1) FileScan parquet [dt#13,price#16,sqft#19L,group#26L,region#27,geoid#28,pct_price#32,pct_sqft#33] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3a://tcceval-data/craigslist_parquet], PartitionFilters: [], PushedFilters: [IsNotNull(pct_price), IsNotNull(pct_sqft), GreaterThan(pct_price,0.02), LessThan(pct_price,0.98)..., ReadSchema: struct<dt:timestamp,price:double,sqft:bigint,group:bigint,region:string,geoid:string,pct_price:do...\n"
     ]
    }
   ],
   "source": [
    "# prepare dataframe by adding the following columns:\n",
    "# - time: 0 for <3/2020, 1 for > 9/2021\n",
    "# - post_treament, which is time*group\n",
    "# we also want to clear the bottom 2% and top 2% from the dataset in both sqft and rent\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "LOW_CUTOFF = F.to_date(F.lit('2020-03-01'))\n",
    "HI_CUTOFF = F.to_date(F.lit('2021-09-01'))\n",
    "\n",
    "did = craigslist.where((craigslist['pct_price'] > 0.02) & (craigslist['pct_price'] < 0.98) & (craigslist['pct_sqft'] > 0.02) & (craigslist['pct_sqft'] < 0.98))\n",
    "did = did.withColumn('time', F.when(did['dt'] < LOW_CUTOFF, 0) \\\n",
    "                            .when(did['dt'] > HI_CUTOFF, 1) \\\n",
    "                            .otherwise(-1))\n",
    "# get rid of the -1's in time column\n",
    "did = did.where(did['time'] != -1)\n",
    "# add post_streatment\n",
    "did = did.withColumn('post_treatment', did['time'] * did['group'])\n",
    "\n",
    "# now group by geoid\n",
    "did.createOrReplaceTempView(\"did\")\n",
    "did = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT percentile(price, 0.5) as med_price, percentile(sqft, 0.5) as med_sqft, FIRST(region) as region,\n",
    "        geoid, time, group, post_treatment\n",
    "    FROM did\n",
    "    GROUP BY geoid, time, group, post_treatment\n",
    "    \"\"\")\n",
    "did = did.withColumn(\"med_price_per_sqft\", did['med_price'] / did['med_sqft'])\n",
    "did.printSchema()\n",
    "did.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e76a9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:===============================================>       (174 + 4) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 ms, sys: 2.61 ms, total: 13.2 ms\n",
      "Wall time: 2.67 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "did = did.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a72a209c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15135/2516189059.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# create design matrix and response vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "predictors = ['time', 'group', 'post_treatment']\n",
    "response = 'med_price_per_sqft'\n",
    "\n",
    "# create design matrix and response vector\n",
    "X = did[predictors]\n",
    "y = did[response]\n",
    "\n",
    "# estimate a simple linear regression model with OLS, using statsmodels\n",
    "model = sm.OLS(y, sm.add_constant(X))\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde4ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_2_4_0",
   "language": "python",
   "name": "spark_2_4_0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
