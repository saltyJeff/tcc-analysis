{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f27dc425",
   "metadata": {},
   "source": [
    "*The following should read Python3.7 and Spark 2.4.0. If not, change the kernel to spark_2_4_0*\n",
    "\n",
    "If no such kernel exists, open a **bash** terminal (the default is plain sh). Then run `setup.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b190f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.12\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.0\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.11.12, OpenJDK 64-Bit Server VM, 1.8.0_312\n",
      "Branch \n",
      "Compiled by user  on 2018-10-29T06:22:05Z\n",
      "Revision \n",
      "Url \n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291417ea",
   "metadata": {},
   "source": [
    "# Export Database\n",
    "The purpose of this script is to dump the contents of the housing-db database.\n",
    "\n",
    "## Options\n",
    "1. `DUMP_PARQUET` Dumps the database in Parquet form. Perfect for importing into another program like R\n",
    "2. `DUMP_EXCEL` Dumps the database in Excel form. Perfect for human analysis after the fac\n",
    "3. `UPLOAD_DEST` Uploads either the parquet, excel, or both files to S3 if `UPLOAD_DEST` is set to a string, or does nothing if set to the empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cb264ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set settings here\n",
    "BUCKET = \"tcceval-data\"\n",
    "DATABASE_CACHE_FOLDER = \"_db_cache\"\n",
    "DUMP_PARQUET_FOLDER = \"craigslist_parquet\"\n",
    "DUMP_EXCEL_FOLDER = \"craigslist_excel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7968fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/aws-java-sdk-core-1.11.835.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/aws-java-sdk-kms-1.11.835.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/aws-java-sdk-s3-1.11.835.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/aws-java-sdk-sagemaker-1.11.835.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/aws-java-sdk-sagemakerruntime-1.11.835.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/aws-java-sdk-sts-1.11.835.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/hadoop-annotations-2.8.1.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/hadoop-auth-2.8.1.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/hadoop-aws-2.8.1.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/hadoop-common-2.8.1.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/htrace-core4-4.0.1-incubating.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/sagemaker-spark_2.11-spark_2.4.0-1.4.2.dev0.jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.sedona#sedona-python-adapter-2.4_2.11 added as a dependency\n",
      "org.apache.sedona#sedona-viz-2.4_2.11 added as a dependency\n",
      "org.datasyslab#geotools-wrapper added as a dependency\n",
      "mysql#mysql-connector-java added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4f2ab544-6275-4f85-9e38-6b9f050d4898;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.sedona#sedona-python-adapter-2.4_2.11;1.2.0-incubating in central\n",
      "\tfound org.locationtech.jts#jts-core;1.18.0 in central\n",
      "\tfound org.wololo#jts2geojson;0.16.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.12.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.12.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.12.2 in central\n",
      "\tfound org.apache.sedona#sedona-core-2.4_2.11;1.2.0-incubating in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.5.0 in central\n",
      "\tfound org.apache.sedona#sedona-sql-2.4_2.11;1.2.0-incubating in central\n",
      "\tfound org.apache.sedona#sedona-viz-2.4_2.11;1.2.0-incubating in central\n",
      "\tfound org.beryx#awt-color-factory;1.0.0 in central\n",
      "\tfound org.datasyslab#geotools-wrapper;1.1.0-25.2 in central\n",
      "\tfound mysql#mysql-connector-java;8.0.30 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.19.4 in central\n",
      ":: resolution report :: resolve 533ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.12.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.12.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.12.2 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.19.4 from central in [default]\n",
      "\tmysql#mysql-connector-java;8.0.30 from central in [default]\n",
      "\torg.apache.sedona#sedona-core-2.4_2.11;1.2.0-incubating from central in [default]\n",
      "\torg.apache.sedona#sedona-python-adapter-2.4_2.11;1.2.0-incubating from central in [default]\n",
      "\torg.apache.sedona#sedona-sql-2.4_2.11;1.2.0-incubating from central in [default]\n",
      "\torg.apache.sedona#sedona-viz-2.4_2.11;1.2.0-incubating from central in [default]\n",
      "\torg.beryx#awt-color-factory;1.0.0 from central in [default]\n",
      "\torg.datasyslab#geotools-wrapper;1.1.0-25.2 from central in [default]\n",
      "\torg.locationtech.jts#jts-core;1.18.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.5.0 from central in [default]\n",
      "\torg.wololo#jts2geojson;0.16.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.locationtech.jts#jts-core;1.18.1 by [org.locationtech.jts#jts-core;1.18.0] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   15  |   0   |   0   |   1   ||   14  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4f2ab544-6275-4f85-9e38-6b9f050d4898\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 14 already retrieved (0kB/12ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-04 23:03:00 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Establishes Spark session\n",
    "ADDITIONAL_ARTIFACTS = [\n",
    "    # Apache Sedona handles spatial queries\n",
    "    'org.apache.sedona:sedona-python-adapter-2.4_2.11:1.2.0-incubating',\n",
    "    'org.apache.sedona:sedona-viz-2.4_2.11:1.2.0-incubating',\n",
    "    'org.datasyslab:geotools-wrapper:1.1.0-25.2',\n",
    "    # JDBC connector for MySQL\n",
    "    'mysql:mysql-connector-java:8.0.30'\n",
    "]\n",
    "\n",
    "import sagemaker_pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "print(classpath)\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.extraClassPath\", classpath) \\\n",
    "    .config(\"spark.executor.extraClassPath\", classpath) \\\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName) \\\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) \\\n",
    "    .config('spark.jars.packages',\n",
    "           ','.join(ADDITIONAL_ARTIFACTS)) \\\n",
    "    .getOrCreate()\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "# suppresses a bunch of AbortableS3Stream warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fcb769b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 1: PROJ: proj_create_from_database: Open of /home/ec2-user/bash/envs/spark_2_4_0/share/proj failed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 158 entries, 0 to 157\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype   \n",
      "---  ------    --------------  -----   \n",
      " 0   group     158 non-null    int64   \n",
      " 1   region    158 non-null    object  \n",
      " 2   geometry  158 non-null    geometry\n",
      " 3   geoid     158 non-null    object  \n",
      "dtypes: geometry(1), int64(1), object(2)\n",
      "memory usage: 5.1+ KB\n",
      "root\n",
      " |-- group: long (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- geoid: string (nullable = true)\n",
      "\n",
      "+-----+------+--------------------+-----------+\n",
      "|group|region|            geometry|      geoid|\n",
      "+-----+------+--------------------+-----------+\n",
      "|    0|fresno|POLYGON ((-119.74...|06019001202|\n",
      "|    0|fresno|POLYGON ((-119.75...|06019001304|\n",
      "|    0|fresno|POLYGON ((-119.73...|06019001407|\n",
      "|    0|fresno|POLYGON ((-119.75...|06019002800|\n",
      "|    0|fresno|POLYGON ((-119.75...|06019003202|\n",
      "|    0|fresno|POLYGON ((-119.86...|06019003807|\n",
      "|    0|fresno|POLYGON ((-119.84...|06019004704|\n",
      "|    0|fresno|POLYGON ((-119.82...|06019004802|\n",
      "|    0|fresno|POLYGON ((-119.79...|06019005100|\n",
      "|    0|fresno|POLYGON ((-119.77...|06019005403|\n",
      "|    1|fresno|POLYGON ((-119.84...|06019000700|\n",
      "|    1|fresno|POLYGON ((-119.79...|06019001100|\n",
      "|    1|fresno|POLYGON ((-119.81...|06019001000|\n",
      "|    1|fresno|POLYGON ((-119.81...|06019000901|\n",
      "|    1|fresno|POLYGON ((-119.81...|06019000200|\n",
      "|    1|fresno|POLYGON ((-119.81...|06019000300|\n",
      "|    1|fresno|POLYGON ((-119.79...|06019000400|\n",
      "|    1|fresno|POLYGON ((-119.80...|06019000600|\n",
      "|    1|fresno|POLYGON ((-119.81...|06019000902|\n",
      "|    1|fresno|POLYGON ((-119.80...|06019000100|\n",
      "+-----+------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Establishes geojson database\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "def load_tract(name):\n",
    "    df = gpd.read_file(f'tracts/{name}.geojson')\n",
    "    df['region'] = name\n",
    "    for col in df.columns:\n",
    "        if col.lower() == 'group':\n",
    "            df['group'] = df[col].astype(int)\n",
    "        elif col.lower() == 'geoid':\n",
    "            df['geoid'] = df[col].astype(str)\n",
    "    \n",
    "    return df[['group', 'region', 'geometry', 'geoid']]\n",
    "\n",
    "tracts_gpd = gpd.GeoDataFrame(pd.concat(map(load_tract, ['fresno', 'la', 'ontario', 'pacoima']), ignore_index=True))\n",
    "tracts_gpd.info()\n",
    "# upload dataframe to spark\n",
    "tracts = spark.createDataFrame(tracts_gpd)\n",
    "tracts.createOrReplaceTempView(\"tracts\")\n",
    "tracts.printSchema()\n",
    "tracts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8ea1740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_parse(str, meth):\n",
    "    if str is None:\n",
    "        return 0\n",
    "    str = str.strip()\n",
    "    str = str.replace(',', '')\n",
    "    if len(str) == 0:\n",
    "        return 0\n",
    "    try:\n",
    "        return meth(str)\n",
    "    except ValueError:\n",
    "        # print(\"Could not turn into number: \"+str)\n",
    "        # i'm getting a bunch of 'buttonclass'\n",
    "        return 0\n",
    "\n",
    "def update_schema(df):\n",
    "    # drop useless columns\n",
    "    df = df.drop(columns=['begts', 'endts', 'region'])\n",
    "    # perform casts\n",
    "    df['pid'] = df['pid'].map(lambda x: try_parse(x, int))\n",
    "    df['repostid'] = df['repostid'].astype(str)\n",
    "    df['lat'] = df['lat'].map(lambda x: try_parse(x, float))\n",
    "    df['lng'] = df['lng'].map(lambda x: try_parse(x, float))\n",
    "    df['accuracy'] = df['accuracy'].map(lambda x: try_parse(x, float))\n",
    "    df['price'] = df['price'].map(lambda x: try_parse(x, float))\n",
    "    df['sqft'] = df['sqft'].map(lambda x: try_parse(x, int))\n",
    "    df['beds'] = df['beds'].map(lambda x: try_parse(x, int))\n",
    "    df['dt'] = pd.to_datetime(df['dt'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e15d279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-04 23:03:09,614 INFO sqlalchemy.engine.Engine SELECT DATABASE()\n",
      "2022-08-04 23:03:09,615 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2022-08-04 23:03:09,620 INFO sqlalchemy.engine.Engine SELECT @@sql_mode\n",
      "2022-08-04 23:03:09,620 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2022-08-04 23:03:09,622 INFO sqlalchemy.engine.Engine SELECT @@lower_case_table_names\n",
      "2022-08-04 23:03:09,623 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2022-08-04 23:03:09,624 INFO sqlalchemy.engine.Engine SHOW TABLE STATUS LIKE 'craigslist_table'\n",
      "2022-08-04 23:03:09,625 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "               Name  Engine  Version Row_format      Rows  Avg_row_length  \\\n",
      "0  craigslist_table  InnoDB       10    Dynamic  10114259            1841   \n",
      "\n",
      "   Data_length  Max_data_length  Index_length  Data_free Auto_increment  \\\n",
      "0  18620596224                0             0    7340032           None   \n",
      "\n",
      "          Create_time         Update_time Check_time          Collation  \\\n",
      "0 2019-10-20 18:24:52 2022-08-04 17:45:28       None  latin1_swedish_ci   \n",
      "\n",
      "  Checksum Create_options Comment  \n",
      "0     None                         \n",
      "TOTAL ROW COUNT (guess) 10114259\n"
     ]
    }
   ],
   "source": [
    "# Connects to database using standard python. Using Spark leads to OOM's\n",
    "# dumps entire database into filesystem\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "TIMEOUT = 100*60*60\n",
    "engine = create_engine('mysql+pymysql://luskincenter:tccproject@housing-site-db.cxxl1so9sozw.us-west-1.rds.amazonaws.com:3306/housing_site_db', echo=True, \n",
    "                       connect_args={'connect_timeout': TIMEOUT})\n",
    "table_status = pd.read_sql_query(\"SHOW TABLE STATUS LIKE 'craigslist_table'\", engine)\n",
    "print(table_status)\n",
    "total_rows = table_status['Rows'][0]\n",
    "print(\"TOTAL ROW COUNT (guess)\", total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "877766b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/ipykernel_launcher.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  after removing the cwd from sys.path.\n",
      "  0%|          | 0/10114259 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-04 23:03:11,614 INFO sqlalchemy.engine.Engine SELECT * FROM craigslist_table\n",
      "2022-08-04 23:03:11,615 INFO sqlalchemy.engine.Engine [raw sql] {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12001136it [15:43, 12718.18it/s]                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 28s, sys: 1min 32s, total: 12min 1s\n",
      "Wall time: 15min 45s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import uuid\n",
    "import shutil\n",
    "from tqdm.autonotebook import tqdm\n",
    "from s3fs import S3FileSystem\n",
    "\n",
    "# delete existing database cache if found\n",
    "s3fs = S3FileSystem()\n",
    "try:\n",
    "    s3fs.rm(f'{BUCKET}/{DATABASE_CACHE_FOLDER}/', recursive=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "rows_written = 0\n",
    "with engine.connect().execution_options(stream_results=True) as conn, tqdm(total=total_rows) as pbar:\n",
    "    for chunk in pd.read_sql_query(\"SELECT * FROM craigslist_table\", conn, chunksize=32*1024):\n",
    "        chunk = update_schema(chunk)\n",
    "        rows_written += len(chunk)\n",
    "        chunk.to_parquet(f's3://{BUCKET}/{DATABASE_CACHE_FOLDER}/part{rows_written}.snappy.parquet', compression='snappy')\n",
    "        \n",
    "        pbar.update(len(chunk))\n",
    "# print(f'TOTAL EXECUTION TIME: {(time.monotonic() - start_time) / 60}m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fa73053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pid: long (nullable = true)\n",
      " |-- repostid: string (nullable = true)\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- neighb: string (nullable = true)\n",
      " |-- beds: long (nullable = true)\n",
      " |-- sqft: long (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lng: double (nullable = true)\n",
      " |-- accuracy: double (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- posttext: string (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------+--------------------+--------------------+------+--------------------+----+----+---------+-----------+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "|       pid|  repostid|                 dt|                 url|               title| price|              neighb|beds|sqft|      lat|        lng|accuracy|             address|            posttext|              domain|            geometry|\n",
      "+----------+----------+-------------------+--------------------+--------------------+------+--------------------+----+----+---------+-----------+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "|7143115084|7112401514|2020-06-19 18:28:00|https://losangele...| Studio on Wyandotte|1552.0|              Reseda|   0|   0|  34.2007|  -118.5391|    22.0|                    |$1552 Normally $1...|https://losangele...|POINT (-118.5391 ...|\n",
      "|7135513277|          |2020-06-19 18:28:00|https://losangele...|BRAND NEW CONSTRU...|3795.0|Miracle Mile Mid-...|   2|1200|  34.0482|  -118.3343|    22.0|                    |BRAND NEW CONSTRU...|https://losangele...|POINT (-118.3343 ...|\n",
      "|7143115297|6905623746|2020-06-19 18:28:00|https://losangele...|1 bedroom/1 bath ...|1915.0|            Van Nuys|   1|   0|  34.1802|  -118.4324|    22.0|                    |1 bedroom/1 bath ...|https://losangele...|POINT (-118.4324 ...|\n",
      "|7143115496|6953354775|2020-06-19 18:28:00|https://losangele...|2 bedroom 2 bath ...|2550.0|                    |   2|   0|  34.1514|  -118.4603|    22.0|                    |2 bedroom 2 bath ...|https://losangele...|POINT (-118.4603 ...|\n",
      "|7145008897|          |2020-06-19 18:28:00|https://losangele...|      House for Rent|4400.0|          Eagle Rock|   4|1800|34.132141|-118.216228|    10.0|4771 Eagle Rock B...|Craftsman House w...|https://losangele...|POINT (-118.21622...|\n",
      "|7143115805|7112424365|2020-06-19 18:28:00|https://losangele...|Beautiful 1 bdrm ...|1995.0|          Northridge|   0|   0|  34.2367|  -118.5466|    22.0|                    |Call Ana for more...|https://losangele...|POINT (-118.5466 ...|\n",
      "|7143615372|7120869466|2020-06-19 18:28:00|https://losangele...|*GREAT 2 BEDROOM!...|1925.0|18429 Prairie St....|   2|   0|34.238903|-118.534619|    10.0|18429 Prairie St....|SUPER 2 BEDROOM, ...|https://losangele...|POINT (-118.53461...|\n",
      "|7143115993|7025412110|2020-06-19 18:28:00|https://losangele...|1 bedroom in Gran...|1830.0|       Granada Hills|   1|   0|  34.2771|  -118.4992|    22.0|                    |1 bedroom/1 bath....|https://losangele...|POINT (-118.4992 ...|\n",
      "|7141785919|5647261028|2020-06-19 18:28:00|https://losangele...|DOWNTOWN  1  BEDROOM|1800.0|        DOWNTOWN L.A|   1| 845|  34.0448|  -118.2434|    22.0|                    |This downtown Los...|https://losangele...|POINT (-118.2434 ...|\n",
      "|7143685815|7112422704|2020-06-19 18:27:00|https://losangele...|  2 bdrm in Van Nuys|2100.0|            Van Nuys|   2|   0|  34.1781|  -118.4574|    22.0|                    |Beautiful 2 bedro...|https://losangele...|POINT (-118.4574 ...|\n",
      "|7143617416|4790121605|2020-06-19 18:27:00|https://losangele...|*SINGLE APARTMENT...|1195.0|18530 Prairie St....|   0|   0|34.238903| -118.53677|    10.0|                    |*SINGLE APARTMENT...|https://losangele...|POINT (-118.53677...|\n",
      "|7143686113|7091736913|2020-06-19 18:27:00|https://losangele...|Studio on Magnoli...|1695.0|        Sherman Oaks|   0|   0|  34.1514|  -118.4603|    22.0|                    |One month free wi...|https://losangele...|POINT (-118.4603 ...|\n",
      "|7145005188|7074729818|2020-06-19 18:27:00|https://losangele...|Newer 1+1 In Enci...|2500.0|              Encino|   1| 678| 34.15837|-118.497276|    10.0|16710 Ventura Bou...|Legado Encino, ne...|https://losangele...|POINT (-118.49727...|\n",
      "|7143686293|7124878791|2020-06-19 18:27:00|https://losangele...|Studio in NOHO Ar...|1500.0|     North Hollywood|   0|   0|  34.1687|  -118.3713|    22.0|                    |Beautiful, fully ...|https://losangele...|POINT (-118.3713 ...|\n",
      "|7145008713|          |2020-06-19 18:27:00|https://losangele...|Top Floor Pool Vi...|1759.0|             Burbank|   0| 532|34.163478|-118.330853|    10.0|311 N Buena Vista...|VIRTUAL TOURS AVA...|https://losangele...|POINT (-118.33085...|\n",
      "|7143692900|          |2020-06-19 18:27:00|https://losangele...|    1 bedroom/1 bath|1695.0|         Canoga Park|   1|   0|  34.2197|  -118.6111|    22.0|                    |1 bedroom/1 bath ...|https://losangele...|POINT (-118.6111 ...|\n",
      "|7143612931|6883475272|2020-06-19 18:27:00|https://losangele...|*1 BDRM! Northrid...|1475.0|18410 Plummer St....|   1|   0|34.242397|-118.534305|     5.0|Plummer St. near ...|Spacious 1 Bedroo...|https://losangele...|POINT (-118.53430...|\n",
      "|7145003216|7100415775|2020-06-19 18:27:00|https://losangele...|1 MONTH FREE! Bev...|2250.0|Burton Way Los An...|   1| 793|34.072109|-118.380919|    10.0|     8660 Burton Way|ONE MONTH FREE on...|https://losangele...|POINT (-118.38091...|\n",
      "|7143698744|          |2020-06-19 18:27:00|https://losangele...|    Near Malibu pier|3300.0|              Malibu|   1|   0|34.071917|-118.849936|    22.0|                    |Beautiful 1 bedro...|https://losangele...|POINT (-118.84993...|\n",
      "|7145008618|          |2020-06-19 18:27:00|https://losangele...|HURRY! Insane Pri...|2795.0|         Los Angeles|   2| 969|34.051486|-118.461933|    10.0|11668 Kiowa near ...|Barrington Kiowa ...|https://losangele...|POINT (-118.46193...|\n",
      "+----------+----------+-------------------+--------------------+--------------------+------+--------------------+----+----+---------+-----------+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 14.3 ms, sys: 1.16 ms, total: 15.5 ms\n",
      "Wall time: 7.13 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load in the craigslist data into Spark\n",
    "craigslist = spark.read.option('mergeSchema', True).parquet(\"s3a://tcceval-data/_db_cache/\")\n",
    "craigslist.createOrReplaceTempView(\"craigslist\")\n",
    "# REMINDER: IT'S LONGITUDE, LATITUDE\n",
    "craigslist = spark.sql(\"SELECT *, ST_POINT(lng, lat) as geometry FROM craigslist\")\n",
    "craigslist.printSchema()\n",
    "craigslist.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4073d097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?i)(accessory\\ dwelling\\s|accessory\\ dwelling\\ unit\\s|ADU\\s|ancillary\\ unit\\s|carriage\\ housecottage\\s|granny\\ flat\\s|granny\\ unit\\s|in\\-law\\s|in\\-law\\ suite\\s|in\\-law\\ unit\\s|Converted\\ garage\\s|Garage\\ conversion\\s|Garage\\ for\\ rent\\s|Casita\\s|Renta\\ garage\\s|Garage\\ convertido\\s|Renta\\ cuarto\\s)\n"
     ]
    }
   ],
   "source": [
    "# define a keyword search regex for detecting ADU's\n",
    "import re\n",
    "adu_keywords = [\n",
    "    'accessory dwelling',\n",
    "    'accessory dwelling unit',\n",
    "    'ADU',\n",
    "    'ancillary unit',\n",
    "    'carriage house'\n",
    "    'cottage',\n",
    "    'granny flat',\n",
    "    'granny unit',\n",
    "    'in-law',\n",
    "    'in-law suite',\n",
    "    'in-law unit',\n",
    "    'Converted garage',\n",
    "    'Garage conversion',\n",
    "    'Garage for rent',\n",
    "    'Casita',\n",
    "    'Renta garage',\n",
    "    'Garage convertido',\n",
    "    'Renta cuarto'\n",
    "]\n",
    "def transform_keywords(kwd):\n",
    "    # escapes each keyword and adds a whitespace marker to the end\n",
    "    return re.escape(kwd)+'\\s'\n",
    "# make case insensitive and union all the keywords\n",
    "ADU_REGEX = '(?i)(' + '|'.join(map(transform_keywords, adu_keywords)) + ')'\n",
    "print(ADU_REGEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb01d470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Window [percent_rank(sqft#40L) windowspecdefinition(region#12, type#267, sqft#40L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS pct_sqft#323], [region#12, type#267], [sqft#40L ASC NULLS FIRST]\n",
      "+- *(5) Sort [region#12 ASC NULLS FIRST, type#267 ASC NULLS FIRST, sqft#40L ASC NULLS FIRST], false, 0\n",
      "   +- Window [percent_rank(price#37) windowspecdefinition(region#12, type#267, price#37 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS pct_price#292], [region#12, type#267], [price#37 ASC NULLS FIRST]\n",
      "      +- *(4) Sort [region#12 ASC NULLS FIRST, type#267 ASC NULLS FIRST, price#37 ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(region#12, type#267, 200)\n",
      "            +- SortAggregate(key=[price#37, address#44, lat#41, lng#42], functions=[first(pid#32L, false), first(repostid#33, false), first(dt#34, false), first(url#35, false), first(title#36, false), first(neighb#38, false), first(beds#39L, false), first(sqft#40L, false), first(accuracy#43, false), first(posttext#45, false), first(domain#46, false), first(group#11L, false), first(region#12, false), first(geoid#14, false), first(url_type#226, false), first(is_adu#246, false), first(type#267, false)])\n",
      "               +- *(3) Sort [price#37 ASC NULLS FIRST, address#44 ASC NULLS FIRST, lat#41 ASC NULLS FIRST, lng#42 ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(price#37, address#44, lat#41, lng#42, 200)\n",
      "                     +- SortAggregate(key=[price#37, address#44, lat#41, lng#42], functions=[partial_first(pid#32L, false), partial_first(repostid#33, false), partial_first(dt#34, false), partial_first(url#35, false), partial_first(title#36, false), partial_first(neighb#38, false), partial_first(beds#39L, false), partial_first(sqft#40L, false), partial_first(accuracy#43, false), partial_first(posttext#45, false), partial_first(domain#46, false), partial_first(group#11L, false), partial_first(region#12, false), partial_first(geoid#14, false), partial_first(url_type#226, false), partial_first(is_adu#246, false), partial_first(type#267, false)])\n",
      "                        +- *(2) Sort [price#37 ASC NULLS FIRST, address#44 ASC NULLS FIRST, lat#41 ASC NULLS FIRST, lng#42 ASC NULLS FIRST], false, 0\n",
      "                           +- *(2) Project [pid#32L, repostid#33, dt#34, url#35, title#36, price#37, neighb#38, beds#39L, sqft#40L, lat#41, lng#42, accuracy#43, address#44, posttext#45, domain#46, group#11L, region#12, geoid#14, regexp_extract(url#35, .*\\/(\\w+)\\/d\\/.*, 1) AS url_type#226, posttext#45 RLIKE (?i)(accessory\\ dwelling\\s|accessory\\ dwelling\\ unit\\s|ADU\\s|ancillary\\ unit\\s|carriage\\ housecottage\\s|granny\\ flat\\s|granny\\ unit\\s|in\\-law\\s|in\\-law\\ suite\\s|in\\-law\\ unit\\s|Converted\\ garage\\s|Garage\\ conversion\\s|Garage\\ for\\ rent\\s|Casita\\s|Renta\\ garage\\s|Garage\\ convertido\\s|Renta\\ cuarto\\s) AS is_adu#246, CASE WHEN (posttext#45 RLIKE (?i)(accessory\\ dwelling\\s|accessory\\ dwelling\\ unit\\s|ADU\\s|ancillary\\ unit\\s|carriage\\ housecottage\\s|granny\\ flat\\s|granny\\ unit\\s|in\\-law\\s|in\\-law\\ suite\\s|in\\-law\\ unit\\s|Converted\\ garage\\s|Garage\\ conversion\\s|Garage\\ for\\ rent\\s|Casita\\s|Renta\\ garage\\s|Garage\\ convertido\\s|Renta\\ cuarto\\s) = true) THEN adu ELSE regexp_extract(url#35, .*\\/(\\w+)\\/d\\/.*, 1) END AS type#267]\n",
      "                              +- BroadcastIndexJoin geometry#62: geometry, RightSide, RightSide, false ST_Contains(geometry#13, geometry#62)\n",
      "                                 :- Project [pid#32L, repostid#33, dt#34, url#35, title#36, price#37, neighb#38, beds#39L, sqft#40L, lat#41, lng#42, accuracy#43, address#44, posttext#45, domain#46, st_point(lng#42, lat#41) AS geometry#62]\n",
      "                                 :  +- *(1) Filter (((isnotnull(price#37) && isnotnull(sqft#40L)) && (price#37 > 0.0)) && (sqft#40L > 0))\n",
      "                                 :     +- *(1) FileScan parquet [pid#32L,repostid#33,dt#34,url#35,title#36,price#37,neighb#38,beds#39L,sqft#40L,lat#41,lng#42,accuracy#43,address#44,posttext#45,domain#46] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3a://tcceval-data/_db_cache], PartitionFilters: [], PushedFilters: [IsNotNull(price), IsNotNull(sqft), GreaterThan(price,0.0), GreaterThan(sqft,0)], ReadSchema: struct<pid:bigint,repostid:string,dt:timestamp,url:string,title:string,price:double,neighb:string...\n",
      "                                 +- SpatialIndex geometry#13: geometry, QUADTREE\n",
      "                                    +- Scan ExistingRDD[group#11L,region#12,geometry#13,geoid#14]\n",
      "root\n",
      " |-- pid: long (nullable = true)\n",
      " |-- repostid: string (nullable = true)\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- neighb: string (nullable = true)\n",
      " |-- beds: long (nullable = true)\n",
      " |-- sqft: long (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lng: double (nullable = true)\n",
      " |-- accuracy: double (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- posttext: string (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- group: long (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- geoid: string (nullable = true)\n",
      " |-- url_type: string (nullable = true)\n",
      " |-- is_adu: boolean (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- pct_price: double (nullable = true)\n",
      " |-- pct_sqft: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a new dataframe, \"properties\", that has all the columns of interest\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# tracts << craigslist, so we broadcast tracts for the join\n",
    "# drop geometry columns since they won't serialize well    \n",
    "properties = craigslist.alias('craigslist').join(F.broadcast(tracts).alias('tracts'), F.expr(\"ST_CONTAINS(tracts.geometry, craigslist.geometry)\")) \\\n",
    "    .drop('craigslist.geometry') \\\n",
    "    .drop('geometry')\n",
    "properties = properties \\\n",
    "    .where(F.expr('craigslist.price > 0 AND craigslist.sqft > 0'))\n",
    "\n",
    "# we want to use either the URL defined shelter type, or the adu type if our regex gets a match\n",
    "properties = properties.withColumn('url_type', F.regexp_extract('url', r'.*\\/(\\w+)\\/d\\/.*', 1))\n",
    "properties = properties.withColumn('is_adu', F.col('posttext').rlike(ADU_REGEX))\n",
    "properties = properties.withColumn(\"type\",\n",
    "  F.when(F.col(\"is_adu\") == F.lit(True), F.lit('adu')) \\\n",
    "    .otherwise(F.col(\"url_type\"))\n",
    ")\n",
    "\n",
    "# counter landlord spam\n",
    "properties = properties.dropDuplicates(['pid'])\n",
    "properties = properties.dropDuplicates(['price', 'address', 'lat', 'lng'])\n",
    "\n",
    "# calculate percentiles over region and type for use in removing outliers\n",
    "properties = properties.withColumn('pct_price', F.percent_rank().over(Window.partitionBy('region', 'type').orderBy('price')))\n",
    "properties = properties.withColumn('pct_sqft', F.percent_rank().over(Window.partitionBy('region', 'type').orderBy('sqft')))\n",
    "\n",
    "properties.createOrReplaceTempView(\"properties\")\n",
    "properties.explain()\n",
    "properties.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "273ea7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 132 ms, sys: 35.5 ms, total: 167 ms\n",
      "Wall time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if DUMP_PARQUET_FOLDER is None:\n",
    "    exit()\n",
    "# delete DUMP_PARQUET_FOLDER if exists\n",
    "try:\n",
    "    s3fs.rm(f'{BUCKET}/{DUMP_PARQUET_FOLDER}', recursive=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "properties = properties.repartition(12)\n",
    "properties.write \\\n",
    "    .option(\"compression\", \"gzip\") \\\n",
    "    .parquet(f's3a://{BUCKET}/{DUMP_PARQUET_FOLDER}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96af80cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[pid: bigint, repostid: string, dt: timestamp, url: string, title: string, price: double, neighb: string, beds: bigint, sqft: bigint, lat: double, lng: double, accuracy: double, address: string, posttext: string, domain: string, group: bigint, region: string, geoid: string, url_type: string, is_adu: boolean, type: string, pct_price: double, pct_sqft: double]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "if DUMP_EXCEL_FOLDER is None:\n",
    "    exit()\n",
    "# delete DUMP_PARQUET_FOLDER if exists\n",
    "try:\n",
    "    s3fs.rm(f'{BUCKET}/{DUMP_EXCEL_FOLDER}', recursive=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "SHELTER_TYPES_ALIAS = {\n",
    "    'reo': 'Real_Estate_Owner',\n",
    "    'sub': 'Sublet_Temporary',\n",
    "    'apa': 'Apartment',\n",
    "    'reb': 'Real_Estate_Broker',\n",
    "    'roo': 'Rooms_Shares',\n",
    "    'vac': 'Vacation_Rentals',\n",
    "    'adu': 'Auxiliary_Dwelling_Unit'\n",
    "}\n",
    "prop_df = properties.repartition(\"region\", \"type\")\n",
    "prop_df.cache()\n",
    "for region in tracts_gpd['region'].unique():\n",
    "    for shelter_type in SHELTER_TYPES_ALIAS.keys():\n",
    "        df = prop_df.filter( (prop_df['region'] == region) & (prop_df['type'] == shelter_type) )\n",
    "        df = df.toPandas()\n",
    "        df.to_excel(f\"s3://{BUCKET}/{DUMP_EXCEL_FOLDER}/{region}-{SHELTER_TYPES_ALIAS[shelter_type]}.xlsx\")\n",
    "prop_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479b4b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5c4fde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_2_4_0",
   "language": "python",
   "name": "spark_2_4_0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
