{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e672499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.12\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.0\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.11.12, OpenJDK 64-Bit Server VM, 1.8.0_312\n",
      "Branch \n",
      "Compiled by user  on 2018-10-29T06:22:05Z\n",
      "Revision \n",
      "Url \n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b822b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set settings here\n",
    "BUCKET = \"tcceval-data\"\n",
    "DATABASE_CACHE_FOLDER = \"_db_cache\"\n",
    "DUMP_PARQUET_FOLDER = \"craigslist_parquet\"\n",
    "DUMP_EXCEL_FOLDER = \"craigslist_excel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "691ac0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/aws-java-sdk-core-1.11.835.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/aws-java-sdk-kms-1.11.835.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/aws-java-sdk-s3-1.11.835.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/aws-java-sdk-sagemaker-1.11.835.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/aws-java-sdk-sagemakerruntime-1.11.835.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/aws-java-sdk-sts-1.11.835.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/hadoop-annotations-2.8.1.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/hadoop-auth-2.8.1.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/hadoop-aws-2.8.1.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/hadoop-common-2.8.1.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/htrace-core4-4.0.1-incubating.jar:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/sagemaker_pyspark/jars/sagemaker-spark_2.11-spark_2.4.0-1.4.2.dev0.jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.sedona#sedona-python-adapter-2.4_2.11 added as a dependency\n",
      "org.apache.sedona#sedona-viz-2.4_2.11 added as a dependency\n",
      "org.datasyslab#geotools-wrapper added as a dependency\n",
      "mysql#mysql-connector-java added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-00a52ad2-2359-4918-96ee-ecf121f43366;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.sedona#sedona-python-adapter-2.4_2.11;1.2.0-incubating in central\n",
      "\tfound org.locationtech.jts#jts-core;1.18.0 in central\n",
      "\tfound org.wololo#jts2geojson;0.16.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.12.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.12.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.12.2 in central\n",
      "\tfound org.apache.sedona#sedona-core-2.4_2.11;1.2.0-incubating in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.5.0 in central\n",
      "\tfound org.apache.sedona#sedona-sql-2.4_2.11;1.2.0-incubating in central\n",
      "\tfound org.apache.sedona#sedona-viz-2.4_2.11;1.2.0-incubating in central\n",
      "\tfound org.beryx#awt-color-factory;1.0.0 in central\n",
      "\tfound org.datasyslab#geotools-wrapper;1.1.0-25.2 in central\n",
      "\tfound mysql#mysql-connector-java;8.0.30 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.19.4 in central\n",
      ":: resolution report :: resolve 686ms :: artifacts dl 13ms\n",
      "\t:: modules in use:\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.12.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.12.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.12.2 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.19.4 from central in [default]\n",
      "\tmysql#mysql-connector-java;8.0.30 from central in [default]\n",
      "\torg.apache.sedona#sedona-core-2.4_2.11;1.2.0-incubating from central in [default]\n",
      "\torg.apache.sedona#sedona-python-adapter-2.4_2.11;1.2.0-incubating from central in [default]\n",
      "\torg.apache.sedona#sedona-sql-2.4_2.11;1.2.0-incubating from central in [default]\n",
      "\torg.apache.sedona#sedona-viz-2.4_2.11;1.2.0-incubating from central in [default]\n",
      "\torg.beryx#awt-color-factory;1.0.0 from central in [default]\n",
      "\torg.datasyslab#geotools-wrapper;1.1.0-25.2 from central in [default]\n",
      "\torg.locationtech.jts#jts-core;1.18.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.5.0 from central in [default]\n",
      "\torg.wololo#jts2geojson;0.16.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.locationtech.jts#jts-core;1.18.1 by [org.locationtech.jts#jts-core;1.18.0] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   15  |   0   |   0   |   1   ||   14  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-00a52ad2-2359-4918-96ee-ecf121f43366\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 14 already retrieved (0kB/10ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-06 19:48:45 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-06 19:48:46 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2022-09-06 19:48:46 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Establishes Spark session\n",
    "ADDITIONAL_ARTIFACTS = [\n",
    "    # Apache Sedona handles spatial queries\n",
    "    'org.apache.sedona:sedona-python-adapter-2.4_2.11:1.2.0-incubating',\n",
    "    'org.apache.sedona:sedona-viz-2.4_2.11:1.2.0-incubating',\n",
    "    'org.datasyslab:geotools-wrapper:1.1.0-25.2',\n",
    "    # JDBC connector for MySQL\n",
    "    'mysql:mysql-connector-java:8.0.30'\n",
    "]\n",
    "\n",
    "import sagemaker_pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "print(classpath)\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.extraClassPath\", classpath) \\\n",
    "    .config(\"spark.executor.extraClassPath\", classpath) \\\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName) \\\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) \\\n",
    "    .config('spark.jars.packages',\n",
    "           ','.join(ADDITIONAL_ARTIFACTS)) \\\n",
    "    .getOrCreate()\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "# suppresses a bunch of AbortableS3Stream warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f657ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pid: long (nullable = true)\n",
      " |-- repostid: string (nullable = true)\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- neighb: string (nullable = true)\n",
      " |-- beds: long (nullable = true)\n",
      " |-- sqft: long (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lng: double (nullable = true)\n",
      " |-- accuracy: double (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- posttext: string (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- group: long (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- geoid: string (nullable = true)\n",
      " |-- url_type: string (nullable = true)\n",
      " |-- is_adu: boolean (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- pct_price: double (nullable = true)\n",
      " |-- pct_sqft: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------+--------------------+--------------------+------+--------------------+----+----+---------+-----------+--------+--------------------+--------------------+--------------------+-----+-------+-----------+--------+------+----+-------------------+-------------------+\n",
      "|       pid|  repostid|                 dt|                 url|               title| price|              neighb|beds|sqft|      lat|        lng|accuracy|             address|            posttext|              domain|group| region|      geoid|url_type|is_adu|type|          pct_price|           pct_sqft|\n",
      "+----------+----------+-------------------+--------------------+--------------------+------+--------------------+----+----+---------+-----------+--------+--------------------+--------------------+--------------------+-----+-------+-----------+--------+------+----+-------------------+-------------------+\n",
      "|7226987246|          |2020-11-16 21:29:00|https://inlandemp...|Una casita (backh...|1500.0|          Ontario CA|   1| 750|34.059511|-117.670188|    20.0|Mountain Ave near...|Una casita (backh...|https://inlandemp...|    1|ontario|06071001600|     apa|  true| adu| 0.3076923076923077|0.23076923076923078|\n",
      "|7466590703|6852445542|2022-04-03 14:36:00|https://inlandemp...|Studio de renta/k...| 965.0|G Mountain/Holt/1...|   0| 300|  34.0584|  -117.6665|    22.0|                    |Disponible para u...|https://inlandemp...|    1|ontario|06071001600|     apa|  true| adu|0.19230769230769232|0.07692307692307693|\n",
      "|7434963978|          |2022-01-22 09:07:00|https://inlandemp...|Una casita (backh...|1800.0|          Ontario CA|   1| 750|34.059511|-117.670188|    20.0|Mountain Ave near...|Una casita (backh...|https://inlandemp...|    1|ontario|06071001600|     apa|  true| adu| 0.4230769230769231|0.23076923076923078|\n",
      "|7165113645|          |2020-07-24 09:21:00|https://inlandemp...|Master bedroom wi...| 700.0|             Fontana|   0| 550|   34.086|   -117.462|    99.0|                    |Master bedroom fo...|https://inlandemp...|    0|ontario|06071002402|     roo| false| roo| 0.4712041884816754| 0.6649214659685864|\n",
      "|7236145481|6951273582|2020-11-23 21:17:00|https://inlandemp...|Room for rent plu...| 650.0|              Rialto|   0| 240|34.108209|-117.361678|    10.0|515 N Acacia Ave ...|Hi Boys...my part...|https://inlandemp...|    0|ontario|06071003803|     roo| false| roo| 0.3193717277486911| 0.4816753926701571|\n",
      "+----------+----------+-------------------+--------------------+--------------------+------+--------------------+----+----+---------+-----------+--------+--------------------+--------------------+--------------------+-----+-------+-----------+--------+------+----+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 7.76 ms, sys: 0 ns, total: 7.76 ms\n",
      "Wall time: 3.44 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Est. 50 sec\n",
    "# load in the craigslist data\n",
    "craigslist = spark.read.parquet(f\"s3a://{BUCKET}/{DUMP_PARQUET_FOLDER}/\")\n",
    "craigslist.printSchema()\n",
    "craigslist.createOrReplaceTempView(\"craigslist\")\n",
    "craigslist.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea4125b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- med_price: double (nullable = true)\n",
      " |-- med_sqft: double (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- time: integer (nullable = false)\n",
      " |-- group: long (nullable = true)\n",
      " |-- post_treatment: long (nullable = true)\n",
      " |-- med_price_per_sqft: double (nullable = true)\n",
      "\n",
      "== Physical Plan ==\n",
      "ObjectHashAggregate(keys=[region#27, time#127, group#26L, post_treatment#152L], functions=[percentile(price#16, 0.5, 1, 0, 0), percentile(sqft#19L, 0.5, 1, 0, 0)])\n",
      "+- Exchange hashpartitioning(region#27, time#127, group#26L, post_treatment#152L, 200)\n",
      "   +- ObjectHashAggregate(keys=[region#27, time#127, group#26L, post_treatment#152L], functions=[partial_percentile(price#16, 0.5, 1, 0, 0), partial_percentile(sqft#19L, 0.5, 1, 0, 0)])\n",
      "      +- *(1) Project [price#16, sqft#19L, group#26L, region#27, CASE WHEN (dt#13 < 1583020800000000) THEN 0 WHEN (dt#13 > 1630454400000000) THEN 1 ELSE -1 END AS time#127, (cast(CASE WHEN (dt#13 < 1583020800000000) THEN 0 WHEN (dt#13 > 1630454400000000) THEN 1 ELSE -1 END as bigint) * group#26L) AS post_treatment#152L]\n",
      "         +- *(1) Filter ((((((isnotnull(pct_price#32) && isnotnull(pct_sqft#33)) && (pct_price#32 > 0.02)) && (pct_price#32 < 0.98)) && (pct_sqft#33 > 0.02)) && (pct_sqft#33 < 0.98)) && NOT (CASE WHEN (dt#13 < 1583020800000000) THEN 0 WHEN (dt#13 > 1630454400000000) THEN 1 ELSE -1 END = -1))\n",
      "            +- *(1) FileScan parquet [dt#13,price#16,sqft#19L,group#26L,region#27,pct_price#32,pct_sqft#33] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3a://tcceval-data/craigslist_parquet], PartitionFilters: [], PushedFilters: [IsNotNull(pct_price), IsNotNull(pct_sqft), GreaterThan(pct_price,0.02), LessThan(pct_price,0.98)..., ReadSchema: struct<dt:timestamp,price:double,sqft:bigint,group:bigint,region:string,pct_price:double,pct_sqft...\n"
     ]
    }
   ],
   "source": [
    "# prepare dataframe by adding the following columns:\n",
    "# - time: 0 for <3/2020, 1 for > 9/2021\n",
    "# - post_treament, which is time*group\n",
    "# we also want to clear the bottom 2% and top 2% from the dataset in both sqft and rent\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "LOW_CUTOFF = F.to_date(F.lit('2020-03-01'))\n",
    "HI_CUTOFF = F.to_date(F.lit('2021-09-01'))\n",
    "\n",
    "did = craigslist.where((craigslist['pct_price'] > 0.02) & (craigslist['pct_price'] < 0.98) & (craigslist['pct_sqft'] > 0.02) & (craigslist['pct_sqft'] < 0.98))\n",
    "did = did.withColumn('time', F.when(did['dt'] < LOW_CUTOFF, 0) \\\n",
    "                            .when(did['dt'] > HI_CUTOFF, 1) \\\n",
    "                            .otherwise(-1))\n",
    "# get rid of the -1's in time column\n",
    "did = did.where(did['time'] != -1)\n",
    "# add post_streatment\n",
    "did = did.withColumn('post_treatment', did['time'] * did['group'])\n",
    "\n",
    "# now group by geoid\n",
    "did.createOrReplaceTempView(\"did\")\n",
    "\n",
    "GROUP_BY_REGION = True\n",
    "if GROUP_BY_REGION:\n",
    "    did = spark.sql(\n",
    "        \"\"\"\n",
    "        SELECT percentile(price, 0.5) as med_price, percentile(sqft, 0.5) as med_sqft, region,\n",
    "            time, group, post_treatment\n",
    "        FROM did\n",
    "        GROUP BY region, time, group, post_treatment\n",
    "        \"\"\")\n",
    "else:\n",
    "    did = spark.sql(\n",
    "        \"\"\"\n",
    "        SELECT percentile(price, 0.5) as med_price, percentile(sqft, 0.5) as med_sqft, FIRST(region) as region,\n",
    "            geoid, time, group, post_treatment\n",
    "        FROM did\n",
    "        GROUP BY geoid, time, group, post_treatment\n",
    "        \"\"\")\n",
    "did = did.withColumn(\"med_price_per_sqft\", did['med_price'] / did['med_sqft'])\n",
    "did.printSchema()\n",
    "did.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b789ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:===============================================>       (174 + 4) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 263 ms, sys: 49.5 ms, total: 312 ms\n",
      "Wall time: 3.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "did = did.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a3e1b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:     med_price_per_sqft   R-squared:                       0.047\n",
      "Model:                            OLS   Adj. R-squared:                 -0.191\n",
      "Method:                 Least Squares   F-statistic:                    0.1972\n",
      "Date:                Tue, 06 Sep 2022   Prob (F-statistic):              0.896\n",
      "Time:                        19:49:00   Log-Likelihood:                -9.8382\n",
      "No. Observations:                  16   AIC:                             27.68\n",
      "Df Residuals:                      12   BIC:                             30.77\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==================================================================================\n",
      "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "const              1.8022      0.258      6.975      0.000       1.239       2.365\n",
      "time               0.2001      0.365      0.548      0.594      -0.596       0.996\n",
      "group             -0.0422      0.365     -0.115      0.910      -0.838       0.754\n",
      "post_treatment    -0.0159      0.517     -0.031      0.976      -1.142       1.110\n",
      "==============================================================================\n",
      "Omnibus:                        2.893   Durbin-Watson:                   1.960\n",
      "Prob(Omnibus):                  0.235   Jarque-Bera (JB):                2.203\n",
      "Skew:                          -0.862   Prob(JB):                        0.332\n",
      "Kurtosis:                       2.427   Cond. No.                         6.85\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/bash/envs/spark_2_4_0/lib/python3.7/site-packages/scipy/stats/stats.py:1542: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=16\n",
      "  \"anyway, n=%i\" % int(n))\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "predictors = ['time', 'group', 'post_treatment']\n",
    "response = 'med_price_per_sqft'\n",
    "\n",
    "# create design matrix and response vector\n",
    "X = did[predictors]\n",
    "y = did[response]\n",
    "\n",
    "# estimate a simple linear regression model with OLS, using statsmodels\n",
    "model = sm.OLS(y, sm.add_constant(X))\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e5541a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_2_4_0",
   "language": "python",
   "name": "spark_2_4_0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
